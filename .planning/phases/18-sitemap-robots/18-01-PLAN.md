---
phase: 18-sitemap-robots
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/sitemap.ts
  - src/app/robots.ts
  - src/app/layout.tsx
  - .env.example
  - .env.local
autonomous: true

must_haves:
  truths:
    - "sitemap.xml is accessible and contains all public page URLs"
    - "robots.txt is accessible with crawl rules and sitemap reference"
    - "Cart and confirmation pages have noindex robots meta tags"
    - "Sitemap excludes cart, confirmation, and removed category/product routes"
    - "All sitemap URLs are absolute (not relative paths)"
  artifacts:
    - path: "src/app/sitemap.ts"
      provides: "Dynamic sitemap generation from product catalog and blog posts"
      contains: "MetadataRoute.Sitemap"
    - path: "src/app/robots.ts"
      provides: "Robots.txt with crawl rules and sitemap reference"
      contains: "MetadataRoute.Robots"
    - path: "src/app/layout.tsx"
      provides: "metadataBase for absolute URL generation"
      contains: "metadataBase"
  key_links:
    - from: "src/app/sitemap.ts"
      to: "src/lib/product/catalog.ts"
      via: "getAllProducts() and getProductUrl() imports"
      pattern: "import.*getAllProducts.*getProductUrl.*catalog"
    - from: "src/app/sitemap.ts"
      to: ".velite/posts.json"
      via: "Velite posts import for blog URLs"
      pattern: "import.*posts.*velite"
    - from: "src/app/robots.ts"
      to: "src/app/sitemap.ts"
      via: "sitemap URL reference in robots.txt output"
      pattern: "sitemap.*sitemap\\.xml"
---

<objective>
Create dynamic sitemap.xml and robots.txt using Next.js built-in file conventions, and add metadataBase to root layout for absolute URL generation.

Purpose: Enable search engines to discover and crawl all public pages while excluding transactional pages (cart, confirmation).
Output: sitemap.ts, robots.ts, metadataBase in layout.tsx, NEXT_PUBLIC_BASE_URL env var
</objective>

<execution_context>
@/Users/robinkonijnendijk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/robinkonijnendijk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/app/layout.tsx
@src/lib/product/catalog.ts
@src/lib/product/types.ts
@.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sitemap.ts and robots.ts with metadataBase</name>
  <files>src/app/sitemap.ts, src/app/robots.ts, src/app/layout.tsx, .env.example, .env.local</files>
  <action>
1. **Add NEXT_PUBLIC_BASE_URL to environment files:**
   - `.env.example`: Add `NEXT_PUBLIC_BASE_URL=https://pureblinds.nl` (placeholder domain)
   - `.env.local`: Add `NEXT_PUBLIC_BASE_URL=https://pureblinds.nl` (same placeholder; user will update with real domain before launch)

2. **Add metadataBase to root layout (`src/app/layout.tsx`):**
   - Add `metadataBase: new URL(process.env.NEXT_PUBLIC_BASE_URL || 'https://pureblinds.nl')` as first property inside the existing `export const metadata: Metadata` object.
   - Do NOT change any other metadata properties. Keep existing title, description, openGraph intact.

3. **Create `src/app/sitemap.ts`:**
   - Import `MetadataRoute` from `'next'`
   - Import `getAllProducts`, `getProductUrl` from `'@/lib/product/catalog'`
   - Import `posts` from `'../../.velite'` (Velite content output)
   - Export default function `sitemap(): MetadataRoute.Sitemap`
   - Build baseUrl from `process.env.NEXT_PUBLIC_BASE_URL || 'https://pureblinds.nl'`
   - Include `lastmod` on all entries. Omit `changefreq` and `priority` (Google ignores them).
   - Static pages (all with `lastModified: new Date()`):
     - Homepage: `${baseUrl}`
     - Products overview: `${baseUrl}/producten`
     - Category page: `${baseUrl}/producten/rolgordijnen`
     - Subcategory — transparent: `${baseUrl}/producten/rolgordijnen/transparante-rolgordijnen`
     - Subcategory — blackout: `${baseUrl}/producten/rolgordijnen/verduisterende-rolgordijnen`
     - Blog listing: `${baseUrl}/blog`
   - Product detail pages: Map over `getAllProducts()`, use `getProductUrl(product)` for path, `lastModified: new Date()`
   - Blog posts: Map over `posts`, use `post.permalink` for path, `lastModified: new Date(post.date)` for accurate per-post dates
   - Return flat array: `[...staticPages, ...productPages, ...blogPages]`
   - Do NOT include `/cart`, `/bevestiging`, or any removed category routes

4. **Create `src/app/robots.ts`:**
   - Import `MetadataRoute` from `'next'`
   - Export default function `robots(): MetadataRoute.Robots`
   - Build baseUrl from `process.env.NEXT_PUBLIC_BASE_URL || 'https://pureblinds.nl'`
   - Rules: `userAgent: '*'`, `allow: '/'`, `disallow: ['/api/', '/_next/', '/cart', '/bevestiging']`
   - Sitemap reference: `sitemap: '${baseUrl}/sitemap.xml'`

**NOTE:** Cart and confirmation already have `robots: { index: false }` in their metadata exports (cart/layout.tsx and confirmation/page.tsx). No changes needed there — verified during planning.
  </action>
  <verify>
Run `npx next build` to confirm no TypeScript errors and sitemap/robots routes compile correctly. Then verify:
- `curl http://localhost:3000/sitemap.xml` returns valid XML with all expected URLs
- `curl http://localhost:3000/robots.txt` returns valid robots.txt with Disallow rules and Sitemap reference
- Confirm sitemap does NOT contain /cart or /bevestiging URLs
- Confirm all sitemap URLs are absolute (start with https://)
  </verify>
  <done>
sitemap.xml serves dynamic XML with homepage, products overview, category page, subcategory pages, all product detail pages, blog listing, and all blog posts. robots.txt serves with Disallow for /api/, /_next/, /cart, /bevestiging and references /sitemap.xml. Root layout has metadataBase set. NEXT_PUBLIC_BASE_URL configured in env files.
  </done>
</task>

</tasks>

<verification>
1. `npm run build` succeeds without errors
2. Visit /sitemap.xml — returns XML with all public page URLs, no cart/confirmation/removed routes
3. Visit /robots.txt — returns text with Allow /, Disallow rules, and Sitemap reference
4. All sitemap URLs are fully qualified (https://pureblinds.nl/...)
5. Blog post entries have individual lastModified dates matching their publication dates
6. Cart page still has noindex meta tag (pre-existing, not modified)
7. Confirmation page still has noindex meta tag (pre-existing, not modified)
</verification>

<success_criteria>
- sitemap.xml generated dynamically from product catalog and blog posts
- robots.txt configured with appropriate crawling rules
- Cart and confirmation pages marked with noindex robots meta
- Sitemap excludes cart, confirmation, and removed categories/products
- Sitemap accessible at /sitemap.xml and referenced in robots.txt
- All URLs in sitemap are absolute (not relative)
</success_criteria>

<output>
After completion, create `.planning/phases/18-sitemap-robots/18-01-SUMMARY.md`
</output>
